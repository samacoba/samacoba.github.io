---
layout: post
title: FP32⇒FP16は2倍か？4倍か？
category: blog
tags: はてな 深層学習
---

ポスト京の名前が「富岳（ふがく）」に決まったようで、スペックもだいぶん明らかになってきました。また、昨年PFNが独自のディープラーニング専用チップ(MN-Core)を開発すると発表しました。富岳は汎用計算機で、今回ディープラーニングの計算にも対応しているのに対し、MN-Coreはディープラーニング専用で作られています。ここで、現在ディープラーニングに使われているNVIDIAのGPUとしてTesla P100（話の都合上 Tensor Coreが入っていないP100にしています）を加えて、単体のチップの性能について見てみたいと思います。


| | Tesla P100 | 富岳CPU | MN-Core |
|:---|:---:|:---:|:---:|
| 倍精度(TFLOPS) | 4.7 | 2.7 | 32.8 |
| 単精度(TFLOPS) | 9.3 | 5.4 | 131 |
| 半精度(TFLOPS) | 18.7 | 10.8 | 524 |


チップの大きさの違いはあるはずで、単純にMN-Coreが早いというのは意味はないですが、ここで注目したいのは、倍精度⇒単精度、 単精度⇒半精度の比率です。Tesla P100、富岳の方はそれぞれ2倍になっていますが、MN-Coreはそれぞれ4倍になっています。

ビット数を半分にすると、計算速度は2倍なのか？それとも4倍なのか？  
ちょっと考えてみました。

※CPU・ハードに関して素人なので大きく間違っているかもしれません。

話を簡単にするために整数4bitのaとbをかけてy = a×b することについて考えてみたいと思います。一般にかけ算というのは足し算よりもはるかに大変です。小学校でならった筆算でも、下の位から「1桁ずつ」かけて、最後に全部足すことで計算しました。コンピュータの中でも同じように掛け算は「1桁ずつ」かける必要があるはずで、桁が大きくなればなるほど、計算量は増えるはずです。

![imgae](/images/20190524-enzan1.png)

なので上の図のように、4bit⇒8bit 演算量が**約4倍**増えると考えられます。

4倍の演算量を実行するには演算器を4倍に増やすか、同じ演算器を4回使い回すがどちらかになりますが、使い回すと遅くなるので、演算器が4倍増える ⇒トランジスタ数が4倍増えることになるはずです。

とすると、ビット数が半分になると演算量は1/4になるので、同じ演算器の数だと4倍並列計算できるはずでFP32⇒FP16は4倍であってもいいような気きがしますが、そうならないのはレジスタ側のサイズが関係してくるのかもしれません。

![imgae](/images/20190524-enzan2.png)

上の図のように、4bit⇒8bitの場合、入力(a,b)と出力(y)の値を入れておくレジスタの容量は4倍ではなく、2倍となるはずです。8bit×1セット ⇒ 4bit×2セットの計算はレジスタ数は変わらず、演算量が減るだけなので改造は簡単ですが、8bit×1セット ⇒ 4bit×4セットの計算はレジスタを増やす改造が必要になります。

なので、ビット数が半分にしたとき富岳やTeslaの計算速度が2倍の速度になっているのは、改造が簡単だからかもしれません。

富岳の方は汎用スパコンのため倍精度がメインになるので、改造の少ない2倍で設計したのに対し、MN-Coreの方はディープラーニング専用のため速度が得られる半精度が重要になってくるので、4倍で設計したと考えられます。

ちなみに整数型と浮動小数点型では違いますが、浮動小数点では指数側は足せばいいだけなので、あんまり計算負荷は大きくなく、仮数部のビット数が重要になるはずです。また、例えば単精度と半精度の演算器は「共用」できるかもわかりません。このほか、Tensor CoreやTPUなどでは、行列の積なので乗算と加算を同時行うことや、同じ変数を何回か別の変数にかけるので、レジスタの数を節約できるなどがあると思いますが、不明な点が多くてよくわかりません。

そもそも、CPU自体 4⇒8⇒16⇒32⇒64bit と進化してきたわけで、この辺の詳しい人から言わせればあたり前、もしくは間違ったこと言っているような気がしますが、ディープラーニングから入って逆にたどっているとよくわかない部分だったりします。

ちなみに富岳のスパコンは、2021年に共用予定で倍精度で400ペタFLOPS（公開用の値、実はもっとはやい）とのことです。京が10ペタFLOPSだったのでその40倍になります。また、MN-Coreのクラスタは2020年春に稼働予定で、半精度で2エクサFLOPSの予定です。単純計算するとMN-Coreは倍精度で 2000÷16 = 125ペタFLOPSになり、富岳は半精度だと0.4 × 4 = 1.6エクサFLOPSになります。スパコンとクラスタの違いはあり単純比較はできませんが、富岳の方が規模は大きく、倍精度では高速に計算できるはずです。しかしながら、ディープラーニングに特化した結果、半精度だとMN-Coreの方が高速となるのかもしれません。


| | 富岳スパコン | MN-Coreクラスタ |
|:---|:---:|:---:|:---:|
| 稼働時期 | 2021年 | 2020年春 |
| 倍精度(ペタFLOPS) | 400 | 125 |
| 単精度(ペタFLOPS) | 800 | 500 |
| 半精度(エクサFLOPS) | 1.6 | 2 | 

